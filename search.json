[{"title":"Python：基于selenium爬取科创板审核问询披露","url":"/2023/02/21/科创板审核问询披露/","content":"\n## 1.科创板审核问询披露\n\n科创板试点的注册制改革，强调 “以信息披露为核心”。为在发行上市审核坚持以信息披露为核心，把好上市企业入口质量关，上交所主要采用公开化问询式审核方式，即交易所提出问询的问题，发行人对这些问题进行回复和说明，中介机构对问询事项的核查过程和结论，以“一问一答”的方式及时向市场公开。\n\n文本重点关注如何通过Python的`selenium`库爬取上述审核问询流程披露的原始文件，并在发行公司层面对问询与回复情况进行汇总统计。\n\n## 2.爬虫实战\n### 2.1 安装selenium及浏览器驱动\n\n本文中，我们使用Python中的`selenium`库对网页进行爬取。`selenium`通过创建模拟浏览器的方式进行爬取，可以完全模拟真实用户的动态操作。\n\n在配置好python环境后，打开命令提示符，并键入如下命令安装`selenium`库：\n\n```\npip install selenium\n```\n或者使用国内镜像源安装`selenium`库：\n\n```\npip install selenium -i http://pypi.douban.com/simple/ --trusted-host  pypi.douban.com\n```\n\n接下来，我们需要安装浏览器驱动。此处我们主要讲解windows系统安装chrome浏览器驱动的步骤和方法。其他系统及其他浏览器驱动的安装，请自行网络搜索相关教程。\n\n首先，我们需要确定chrome浏览器版本。打开chrome浏览器，在新标签页输入 chrome://settings/help 进入设置界面，查看目前的版本信息。\n\n![](https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/范思妤_Python爬取科创板审核问询披露文件_Fig1.png)\n\n在获取浏览器版本信息后，需下载对应版本的浏览器驱动。打开chrome浏览器驱动下载地址：[驱动下载地址](http://chromedriver.storage.googleapis.com/index.html)，找到和目前版本最接近的驱动，Windows系统用户需下载win32版本。\n\n![](https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/范思妤_Python爬取科创板审核问询披露文件_Fig2.png)\n\n\n解压驱动文件包得到`chromedriver.exe`，保存到指定路径，并务必将当前路径添加到环境变量中（我的电脑>>右键属性>>高级系统设置>>高级>>环境变量>>系统变量>>Path）。\n\n### 2.2 浏览器驱动配置使用\n\n我们使用如下代码创建浏览器驱动对象。此外，我们可以根据爬虫目标网页调整浏览器选项，提供稳定运行环境。\n\n```python\nfrom selenium import webdriver\nfrom selenium.webdriver import ChromeOptions\n\nCHROME_OPTIONS = webdriver.ChromeOptions()\nprefs = {\"profile.managed_default_content_settings.images\":2}   # 1代表显示图片，2代表不显示图片\nCHROME_OPTIONS.add_experimental_option(\"prefs\", prefs)\nCHROME_OPTIONS.add_experimental_option(\"excludeSwitches\",[\"enable-automation\"]) \nCHROME_OPTIONS.add_experimental_option(\"useAutomationExtension\", False)\nCHROME_OPTIONS.add_argument('--disable-blink-features=AutomationControlled')\nCHROME_DRIVER = r'D:/chromedriver/chromedriver.exe'  # 此处为chromedriver.exe所在路径\n\n# 声明浏览器\ndriver = webdriver.Chrome(executable_path=CHROME_DRIVER, options=CHROME_OPTIONS)  \n\n```\n\n### 2.1 分析网页结构\n首先，我们需要浏览待爬取的页面结构，并检查网页确定网页类型。\n\n上交所科创板股票审核页面如下。最上方一行分类列示已递交申请的发行人的状态，我们的样本仅爬取已经有注册结果的发行人（即已经走完整个科创板审核问询流程）。\n\n所有发行人按照时间由近及远的顺序列示在页面表格中。每行发行人全称对应一个超链接，点击即进入发行人审核问询的详情页面。因此，我们整体的爬虫思路如下：\n- 获取全部发行人对应的链接列表\n- 遍历目标链接列表爬取某一页面的特定信息，并储存到本地。\n\n![](https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/范思妤_Python爬取科创板审核问询披露文件_Fig3.png)\n\n### 2.2 获取url列表\n如上所述，我们仅爬取已经有注册结果的发行人。因此，首先需要用selenium自动化模拟鼠标点击操作，以点击页面上方的“注册结果”，获取跳转后的页面上的信息。\n\n```python\n# 导入所需要的包\nfrom selenium.webdriver.common.by import By\nimport time\n\n# 请求页面\nurl = \"http://kcb.sse.com.cn/renewal/\"\ndriver.get(url)           \n\n# 通过Xpath定位到“注册结果”，并点击，等待页面加载\ndriver.find_element(By.XPATH,'//*[@id=\"select5\"]/a/span[2]').click()\ntime.sleep(3)\n```\n\n通过加载后的检查页面，我们发现url就包含在发行人全称的href属性中。我们进行如下步骤的操作以获取完整的发行人url列表：\n\n- 定位发行人全称，并提取元素的href属性值\n- 定位“下一页”按钮，模拟鼠标操作进行翻页\n- 重复以上两步，直至最后一页\n\nselenium提供了多种元素定位方式，如xpath，id，name等。在本文中，我们用Xpath定位元素。具体步骤如下图，得到对应元素的Xpath后，通过语法`@href`选取元素属性。\n\n![](https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/范思妤_Python爬取科创板审核问询披露文件_Fig4.png)\n\n\n代码如下：\n\n```python\n\n# Xpath定位元素\nall_url_list = []\nurl_xpath = '//*[@id=\"dataList1_container\"]/tbody/tr/td[2]/a/@href'\nnext_page = '//*[@id=\"dataList1_container_next\"]/span'\n\n# 获取发行人url列表\nfor i in range(1,28):        # 最大页数是28\n    print(\"正在抓取第%s页的内容\" %i)\n    html = driver.page_source\n    tree = etree.HTML(html)\n    url_list = tree.xpath(url_xpath)\n    all_url_list.extend(url_list)\n    time.sleep(1)\n    if i<=26 : \n        # 点击下一页\n        driver.find_element(By.XPATH,next_page).click()   \n        time.sleep(2)        # 等待页面加载\n    else:         \n        pass                 # 最后一页没有“下一页”\ndriver.quit()\n\n# 打印最后10个url，检查是否所有url爬取成功\nfor url in all_url_list[-10:]:\n    print(url)\n```\n### 2.3 遍历url列表爬取审核问询披露\n\n在获取了全部发行人url列表之后，我们需要遍历url列表，获取特定发行人审核问询的详情页面。\n\n在这一页面，我们需要爬取的关键内容如下：\n\n1. 发行人基本信息：\n   - 公司全称（fullname）\n   - 公司简称（shortname）\n   - 保荐机构（broker）\n   - 会计师事务所（auditor）\n   - 律师事务所（lawyer）\n   - 受理日期（acceptdate）\n   - 首次问询日期（inquirydate）\n   - 上市委会议通过日期（passdate）\n\n![](https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/范思妤_Python爬取科创板审核问询披露文件_Fig5.png)\n\n2. 审核问询披露文件：\n   - 发行人及保荐机构回复意见\n   - 会计师回复意见\n   - 法律意见书\n\n![](https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/范思妤_Python爬取科创板审核问询披露文件_Fig6.png)\n\n代码如下：\n\n```python\n\n#————爬取每个url对应的科创板发行上市公司页面————#\n\n# 导入所需的包\nimport time\nfrom lxml import etree\nimport pandas as pd\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nimport requests\nimport os\nfrom urllib.request import urlretrieve\n\n# 在路径下创建文件夹\nos.mkdir(r'./companies')\n\n# 浏览器驱动配置及使用\nCHROME_OPTIONS = webdriver.ChromeOptions()\nprefs = {\"profile.managed_default_content_settings.images\":2}   # 1代表显示图片，2代表不显示图片\nCHROME_OPTIONS.add_experimental_option(\"prefs\", prefs)\nCHROME_DRIVER = r'D:/chromedriver/chromedriver.exe'\ndriver = webdriver.Chrome(executable_path=CHROME_DRIVER, options=CHROME_OPTIONS)\n\n# 发行人基本信息的Xpath定位\nfullname_xpath        = '//*[@id=\"issuer_full\"]'\nshortname_xpath       = '//*[@id=\"issuer_sec\"]'\nbroker_xpath          = '//*[@id=\"sponsor_org\"]/a'\nauditor_xpath         = '//*[@id=\"accounts_org\"]/a'\nlawyer_xpath          = '//*[@id=\"law_firm\"]/a'\nacceptdate_xpath      = '//*[@id=\"step1F\"]/div'\ninquirydate_xpath     = '//*[@id=\"step2F\"]/div'\npassdate_xpath        = '//*[@id=\"step3F\"]/div'\n\n# 注册制审核问询披露文件的Xpath定位\nSECLetter_xpath       = '//*[@id=\"yjhf\"]/tbody/tr/td[2]/a'\n\n# 创建空列表以盛放后续爬虫信息\nall_fullname    = []\nall_shortname   = []\nall_broker      = []\nall_auditor     = []\nall_lawyer      = []\nall_acceptdate  = []\nall_inquirydate = []\nall_passdate    = []\nall_num_letter1 = []\nall_num_letter2 = []\nall_num_letter3 = []\n\nfor url_item in all_url_list:\n    url = 'http://kcb.sse.com.cn' + url_item\n    driver.get(url) \n    time.sleep(2)\n    html = driver.page_source\n    tree = etree.HTML(html)\n    \n    # 获取发行人基本信息的文本节点\n    fullname_list    = tree.xpath(fullname_xpath + '/text()')\n    all_fullname.append(fullname_list[0])\n    \n    shortname_list   = tree.xpath(shortname_xpath+ '/text()')\n    all_shortname.append(shortname_list[0])\n    \n    broker_list      = tree.xpath(broker_xpath+ '/text()')\n    content          = ';'.join(broker_list)  # 可能会有多个保荐机构，用分号链接，下同\n    all_broker.append(content)\n    \n    auditor_list     = tree.xpath(auditor_xpath+ '/text()')\n    content          = ';'.join(auditor_list)\n    all_auditor.append(content)\n    \n    lawyer_list      = tree.xpath(lawyer_xpath+ '/text()')\n    content          = ';'.join(lawyer_list)\n    all_lawyer.append(content)\n    \n    acceptdate_list  = tree.xpath(acceptdate_xpath+ '/text()')\n    all_acceptdate.append(acceptdate_list[0])\n    \n    inquirydate_list = tree.xpath(inquirydate_xpath+ '/text()')\n    all_inquirydate.append(inquirydate_list[0])\n    \n    passdate_list    = tree.xpath(passdate_xpath+ '/text()')\n    all_passdate.append(passdate_list[0])\n\n    # 获取问询与回复信息的文本节点\n    SECLetter_list   = tree.xpath(SECLetter_xpath  + '/text()')\n    # 获取问询与回复的href属性，为后续下载对应文件pdf做准备\n    SECLetter_url    = tree.xpath(SECLetter_xpath  + '/@href')\n    \n    # 为每一公司创建子文件夹，用以盛放分类后的文件pdf\n    os.mkdir('./companies/'+fullname_list[0])\n    os.mkdir('./companies/'+fullname_list[0] + '/问询函和回函/')\n    os.mkdir('./companies/'+fullname_list[0] + '/审计意见/')\n    os.mkdir('./companies/'+fullname_list[0] + '/法律意见/')\n    os.mkdir('./companies/'+fullname_list[0] + '/其他/')\n    \n    # 注册制审核问询披露文件的计数算子\n    count_letter1 = 0  \n    count_letter2 = 0\n    count_letter3 = 0\n    \n    #获取pdf文件方法1，用requests.get：\n    for i in range(len(SECLetter_list)):\n        url = SECLetter_url[i]\n        r = requests.get('http:'+ url)\n\n        # 通过文件名称对文件进行分类，并计数\n        if  ('发行人'in SECLetter_list[i]) or ('落实函' in SECLetter_list[i]) and  ('会计' not in SECLetter_list[i]) and ('律师'not in SECLetter_list[i]) and ('法律' not in SECLetter_list[i]):\n            with open('./companies/'+fullname_list[0] + '/问询函和回函/'+SECLetter_list[i]+'.pdf', 'wb+') as f:\n                f.write(r.content)\n                count_letter1 += 1\n        elif  '会计' in SECLetter_list[i]:  \n            with open('./companies/'+fullname_list[0] + '/审计意见/'+SECLetter_list[i]+'.pdf', 'wb+') as f:\n                f.write(r.content)\n                count_letter2 += 1\n        elif ('律师'in SECLetter_list[i]) or ('法律' in SECLetter_list[i]):\n            with open('./companies/'+fullname_list[0] + '/法律意见/'+SECLetter_list[i]+'.pdf', 'wb+') as f:\n                f.write(r.content)\n                count_letter3 += 1\n        else :\n            with open('./companies/'+fullname_list[0] + '/其他/'+SECLetter_list[i]+'.pdf', 'wb+') as f:\n                f.write(r.content)\n    \n    all_num_letter1.append(count_letter1)\n    all_num_letter2.append(count_letter2)\n    all_num_letter3.append(count_letter3)\n        \n    '''\n    # 获取pdf文件方法2，用urlretrieve：\n    # 以下代码仅做方法思路展示，未对pdf进行进一步分类\n    for i in range(len(SECLetter_list)):\n        url = 'http:' + SECLetter_url[i]\n        urlretrieve(url,filename = './科创板/companies/'+fullname_list[0]+'/'+SECLetter_list[i]+'.pdf')\n    '''\ndriver.quit()\n\n# 将爬虫信息储存到本地excel文件\nfile = r\".\\科创板注册制信息披露.xlsx\"\nfinal_data = [all_fullname,all_shortname,all_broker,all_auditor,all_lawyer,all_acceptdate,all_inquirydate,all_passdate,all_num_letter1,all_num_letter2,all_num_letter3]\ndf = pd.DataFrame(final_data).T\ndf.columns = [\"公司全称\", \"公司简称\", \"保荐机构\", \"会计师事务所\", \"律师事务所\",\"受理日期\",\"开始问询日期\",\"上市委通过日期\",\"问询与回函数量\",\"审计意见数量\",\"律师意见数量\"]\ndf.to_excel(file, index = None)\n```\n最终，我们可以得到如下审核问询情况汇总及审核问询披露文件的原始pdf：\n![](https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/范思妤_Python爬取科创板审核问询披露文件_Fig7.png)\n\n![](https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/范思妤_Python爬取科创板审核问询披露文件_Fig8.png)\n\n**注意**：以上代码在计算“问询与回函数量”、“审计意见数量”、“律师意见数量”时仅简单地计数页面上所列示的对应文件的数量，但并不代表发行人实际被问询次数。在现实中，若公司在季度末或者年末更新财务报表，则需根据更新后的财务报表向交易所重新提交以上文件。举个例子：《发行人及保荐机构回复意见》和《发行人及保荐机构回复意见（2022年半年报财务数据更新版）》本质上属于同一份回函的两个版本，计算一次问询次数，而不是两次。此处请读者仔细甄别。","tags":["python"],"categories":["code"]},{"title":"Hello World","url":"/2023/02/21/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","tags":["others"],"categories":["note"]},{"url":"/tag.css","content":"\n<ul>\n  <% site.tags.forEach(function(tag){ %>\n    <li><a href=\"<%- url_for(tag.path) %>\"><%- tag.name %></a></li>\n  <% }); %>\n</ul>"},{"title":"categories","url":"/categories/index.html"},{"url":"/js/search.min.js","content":"function search(){function e(e,t=400){let n=-1;return(...c)=>{clearTimeout(n),n=setTimeout(()=>{e(c)},t)}}function t(){r.innerHTML=\"\",\"\"!=o.value.trim()&&(l.innerHTML=\"\",n.forEach(e=>{if(e.title.toLowerCase().includes(o.value.toLowerCase())){let t=document.createElement(\"a\");t.href=e.url,t.text=e.title;let n=document.createElement(\"li\");n.appendChild(t),l.appendChild(n)}}),r.appendChild(l))}let n;fetch(\"/search.json\").then(e=>e.json()).then(e=>{n=e.concat()}).catch(console.error);let c=e(t),o=document.getElementById(\"search-text\"),r=document.getElementById(\"result\"),l=document.createDocumentFragment();o.oninput=function(){c()}}window.addEventListener(\"load\",search);"},{"title":"Tags","url":"/tags/index.html"}]